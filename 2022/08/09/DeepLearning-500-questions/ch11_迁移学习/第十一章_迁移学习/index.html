<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>第十一章_迁移学习 | HExLL-迷雾日志</title><meta name="keywords" content="Python,DeepLearning,ch11_迁移学习"><meta name="author" content="GreenSeaa"><meta name="copyright" content="GreenSeaa"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="[TOC] 第十一章 迁移学习 ​	本章主要简明地介绍了迁移学习的基本概念、迁移学习的必要性、研究领域和基本方法。重点介绍了几大类常用的迁移学习方法：数据分布自适应方法、特征选择方法、子空间学习方法、以及目前最热门的深度迁移学习方法。除此之外，我们也结合最近的一些研究成果对未来迁移学习进行了一些展望。并提供了一些迁移学习领域的常用学习资源，以方便感兴趣的读者快速开始学习。 11.1 迁移学习基础知">
<meta property="og:type" content="article">
<meta property="og:title" content="第十一章_迁移学习">
<meta property="og:url" content="https://yuanweize.github.io/2022/08/09/DeepLearning-500-questions/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="HExLL-迷雾日志">
<meta property="og:description" content="[TOC] 第十一章 迁移学习 ​	本章主要简明地介绍了迁移学习的基本概念、迁移学习的必要性、研究领域和基本方法。重点介绍了几大类常用的迁移学习方法：数据分布自适应方法、特征选择方法、子空间学习方法、以及目前最热门的深度迁移学习方法。除此之外，我们也结合最近的一些研究成果对未来迁移学习进行了一些展望。并提供了一些迁移学习领域的常用学习资源，以方便感兴趣的读者快速开始学习。 11.1 迁移学习基础知">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7">
<meta property="article:published_time" content="2022-08-09T05:15:44.000Z">
<meta property="article:modified_time" content="2022-08-09T15:08:01.895Z">
<meta property="article:author" content="GreenSeaa">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="ch11_迁移学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://yuanweize.github.io/2022/08/09/DeepLearning-500-questions/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-WD3QXC5CQE"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-WD3QXC5CQE');
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"SW7OVVVFHX","apiKey":"5d05023a8579462e72ea704e4f43cf3f","indexName":"hexo","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '第十一章_迁移学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '09-08-2022 17:08:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s.gravatar.com/avatar/50de7ee8a1fc96ada7495a641400642d?s=512" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">803</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">75</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">72</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://app.plex.tv/"><i class="fa-fw fas fa-video"></i><span> Movie[PLEX]</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://command.yuanweize.win/"><i class="fa-fw fab fa-linux"></i><span> Linux-command</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">HExLL-迷雾日志</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://app.plex.tv/"><i class="fa-fw fas fa-video"></i><span> Movie[PLEX]</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://command.yuanweize.win/"><i class="fa-fw fab fa-linux"></i><span> Linux-command</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">第十一章_迁移学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-09T05:15:44.000Z" title="发表于 09-08-2022 07:15:44">09-08-2022</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-09T15:08:01.895Z" title="更新于 09-08-2022 17:08:01">09-08-2022</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/DeepLearning-500-questions/">DeepLearning-500-questions</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/DeepLearning-500-questions/ch11-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">ch11_迁移学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>55分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="第十一章_迁移学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1>第十一章 迁移学习</h1>
<p>​	本章主要简明地介绍了迁移学习的基本概念、迁移学习的必要性、研究领域和基本方法。重点介绍了几大类常用的迁移学习方法：数据分布自适应方法、特征选择方法、子空间学习方法、以及目前最热门的深度迁移学习方法。除此之外，我们也结合最近的一些研究成果对未来迁移学习进行了一些展望。并提供了一些迁移学习领域的常用学习资源，以方便感兴趣的读者快速开始学习。</p>
<h2 id="11-1-迁移学习基础知识">11.1 迁移学习基础知识</h2>
<h3 id="11-1-1-什么是迁移学习？">11.1.1 什么是迁移学习？</h3>
<p>找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。</p>
<h3 id="11-1-2-为什么需要迁移学习？">11.1.2 为什么需要迁移学习？</h3>
<ol>
<li><strong>大数据与少标注的矛盾</strong>：虽然有大量的数据，但往往都是没有标注的，无法训练机器学习模型。人工进行数据标定太耗时。</li>
<li><strong>大数据与弱计算的矛盾</strong>：普通人无法拥有庞大的数据量与计算资源。因此需要借助于模型的迁移。</li>
<li><strong>普适化模型与个性化需求的矛盾</strong>：即使是在同一个任务上，一个模型也往往难以满足每个人的个性化需求，比如特定的隐私设置。这就需要在不同人之间做模型的适配。</li>
<li><strong>特定应用（如冷启动）的需求</strong>。</li>
</ol>
<h3 id="11-1-3-迁移学习的基本问题有哪些？">11.1.3 迁移学习的基本问题有哪些？</h3>
<p>基本问题主要有3个：</p>
<ul>
<li><strong>How to transfer</strong>： 如何进行迁移学习？（设计迁移方法）</li>
<li><strong>What to transfer</strong>： 给定一个目标领域，如何找到相对应的源领域，然后进行迁移？（源领域选择）</li>
<li><strong>When to transfer</strong>： 什么时候可以进行迁移，什么时候不可以？（避免负迁移）</li>
</ul>
<h3 id="11-1-4-迁移学习有哪些常用概念？">11.1.4 迁移学习有哪些常用概念？</h3>
<ul>
<li>基本定义
<ul>
<li><strong>域(Domain)</strong>：数据特征和特征分布组成，是学习的主体
<ul>
<li><strong>源域 (Source domain)</strong>：已有知识的域</li>
<li><strong>目标域 (Target domain)</strong>：要进行学习的域</li>
</ul>
</li>
<li><strong>任务 (Task)</strong>：由目标函数和学习结果组成，是学习的结果</li>
</ul>
</li>
<li>按特征空间分类
<ul>
<li><strong>同构迁移学习（Homogeneous TL）</strong>： 源域和目标域的特征空间相同，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>s</mi></msub><mo>=</mo><msub><mi>D</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">D_s=D_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
<li><strong>异构迁移学习（Heterogeneous TL）</strong>：源域和目标域的特征空间不同，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>s</mi></msub><mo mathvariant="normal">≠</mo><msub><mi>D</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">D_s\ne D_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li>按迁移情景分类
<ul>
<li><strong>归纳式迁移学习（Inductive TL）</strong>：源域和目标域的学习任务不同</li>
<li><strong>直推式迁移学习（Transductive TL)</strong>：源域和目标域不同，学习任务相同</li>
<li><strong>无监督迁移学习（Unsupervised TL)</strong>：源域和目标域均没有标签</li>
</ul>
</li>
<li>按迁移方法分类
<ul>
<li><strong>基于实例的迁移 (Instance based TL)</strong>：通过权重重用源域和目标域的样例进行迁移</li>
<li><strong>基于特征的迁移 (Feature based TL)</strong>：将源域和目标域的特征变换到相同空间</li>
<li><strong>基于模型的迁移 (Parameter based TL)</strong>：利用源域和目标域的参数共享模型</li>
<li><strong>基于关系的迁移 (Relation based TL)</strong>：利用源域中的逻辑网络关系进行迁移</li>
</ul>
</li>
</ul>
<p><img src="img/ch11/1542972502781.png" alt="1542972502781"></p>
<p><img src="img/ch11/1542974131814.png" alt="1542974131814"></p>
<h3 id="11-1-5-迁移学习与传统机器学习有什么区别？">11.1.5 迁移学习与传统机器学习有什么区别？</h3>
<table>
<thead>
<tr>
<th></th>
<th>迁移学习</th>
<th>传统机器学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据分布</td>
<td>训练和测试数据不需要同分布</td>
<td>训练和测试数据同分布</td>
</tr>
<tr>
<td>数据标签</td>
<td>不需要足够的数据标注</td>
<td>足够的数据标注</td>
</tr>
<tr>
<td>建模</td>
<td>可以重用之前的模型</td>
<td>每个任务分别建模</td>
</tr>
</tbody>
</table>
<p><img src="img/ch11/1542973960796.png" alt="1542973960796"></p>
<h3 id="11-1-6-迁移学习的核心及度量准则？">11.1.6 迁移学习的核心及度量准则？</h3>
<p><strong>迁移学习的总体思路可以概括为</strong>：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。</p>
<p><strong>迁移学习的核心是</strong>：找到源领域和目标领域之间的相似性，并加以合理利用。这种相似性非常普遍。比如，不同人的身体构造是相似的；自行车和摩托车的骑行方式是相似的；国际象棋和中国象棋是相似的；羽毛球和网球的打球方式是相似的。这种相似性也可以理解为不变量。以不变应万变，才能立于不败之地。</p>
<p>**有了这种相似性后，下一步工作就是， 如何度量和利用这种相似性。**度量工作的目标有两点：一是很好地度量两个领域的相似性，不仅定性地告诉我们它们是否相似，更定量地给出相似程度。二是以度量为准则，通过我们所要采用的学习手段，增大两个领域之间的相似性，从而完成迁移学习。</p>
<p><strong>一句话总结： 相似性是核心，度量准则是重要手段。</strong></p>
<h3 id="11-1-7-迁移学习与其他概念的区别？">11.1.7 迁移学习与其他概念的区别？</h3>
<ol>
<li>迁移学习与多任务学习关系：
<ul>
<li><strong>多任务学习</strong>：多个相关任务一起协同学习；</li>
<li><strong>迁移学习</strong>：强调信息复用，从一个领域(domain)迁移到另一个领域。</li>
</ul>
</li>
<li>迁移学习与领域自适应：<strong>领域自适应</strong>：使两个特征分布不一致的domain一致。</li>
<li>迁移学习与协方差漂移：<strong>协方差漂移</strong>：数据的条件概率分布发生变化。</li>
</ol>
<p>Reference：</p>
<ol>
<li><a href="https%EF%BC%9A//github.com/jindongwang/transferlearning-tutorial">王晋东，迁移学习简明手册</a></li>
<li>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175.</li>
<li>Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM.</li>
</ol>
<h3 id="11-1-8-什么是负迁移？产生负迁移的原因有哪些？">11.1.8 什么是负迁移？产生负迁移的原因有哪些？</h3>
<p>负迁移(Negative Transfer)指的是，在源域上学习到的知识，对于目标域上的学习产生负面作用。</p>
<p>产生负迁移的原因主要有：</p>
<ul>
<li>数据问题：源域和目标域压根不相似，谈何迁移？</li>
<li>方法问题：源域和目标域是相似的，但是，迁移学习方法不够好，没找到可迁移的成分。</li>
</ul>
<p>负迁移给迁移学习的研究和应用带来了负面影响。在实际应用中，找到合理的相似性，并且选择或开发合理的迁移学习方法，能够避免负迁移现象。</p>
<h3 id="11-1-9-迁移学习的基本思路？">11.1.9 迁移学习的基本思路？</h3>
<p>迁移学习的总体思路可以概括为：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习。</p>
<ol>
<li>找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。</li>
<li>迁移学习，是指利用数据、任务、或模型之间的相似性，将在旧领域学习过的模型，应用于新领域的一种学习过程。</li>
<li>迁移学习<strong>最有用的场合</strong>是，如果你尝试优化任务B的性能，通常这个任务数据相对较少。<br>
例如，在放射科中你知道很难收集很多射线扫描图来搭建一个性能良好的放射科诊断系统，所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用 1 百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务在放射科任务上做得更好，尽管任务没有这么多数据。</li>
<li>迁移学习什么时候是有意义的？它确实可以<strong>显著提高</strong>你的<strong>学习任务的性能</strong>，但我有时候也见过有些场合使用迁移学习时，任务实际上数据量比任务要少， 这种情况下增益可能不多。</li>
</ol>
<blockquote>
<p>什么情况下可以使用迁移学习？</p>
<p>假如两个领域之间的区别特别的大，<strong>不可以直接采用迁移学习</strong>，因为在这种情况下效果不是很好。在这种情况下，推荐使用[3]的工作，在两个相似度很低的domain之间一步步迁移过去（踩着石头过河）。</p>
</blockquote>
<blockquote>
<ol>
<li>迁移学习主要解决方案有哪些？</li>
<li>除直接看infer的结果的Accurancy以外，如何衡量迁移学习学习效果？</li>
<li>对抗网络是如何进行迁移的？</li>
</ol>
</blockquote>
<p>Reference：</p>
<ol>
<li><a href="https%EF%BC%9A//github.com/jindongwang/transferlearning-tutorial">王晋东，迁移学习简明手册</a></li>
<li>Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., &amp; Vaughan, J. W. (2010). A theory of learning from different domains. Machine learning, 79(1-2), 151-175.</li>
<li>Tan, B., Song, Y., Zhong, E. and Yang, Q., 2015, August. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1155-1164). ACM.</li>
</ol>
<h2 id="11-2-迁移学习的基本思路有哪些？">11.2 迁移学习的基本思路有哪些？</h2>
<p>​	迁移学习的基本方法可以分为四种。这四种基本的方法分别是：基于样本的迁移， 基于模型 的迁移， 基于特征的迁移，及基于关系的迁移。</p>
<h3 id="11-2-1-基于样本迁移">11.2.1 基于样本迁移</h3>
<p>​	基于样本的迁移学习方法 (Instance based Transfer Learning) 根据一定的权重生成规则，对数据样本进行重用，来进行迁移学习。图<a href="#bookmark90">14</a>形象地表示了基于样本迁移方法的思想源域中存在不同种类的动物，如狗、鸟、猫等，目标域只有狗这一种类别。在迁移时，为了最大限度地和目标域相似，我们可以人为地提高源域中属于狗这个类别的样本权重。</p>
<p><img src="media/631e5aab4e0680c374793804817bfbb6.jpg" alt=""></p>
<center>图 14: 基于样本的迁移学习方法示意图
<p>​	在迁移学习中，对于源域D~s~和目标域D~t~，通常假定产生它们的概率分布是不同且未知的(P(X~s~) =P(X~t~))。另外，由于实例的维度和数量通常都非常大，因此，直接对 P(X~s~) 和P(X~t~) 进行估计是不可行的。因而，大量的研究工作 [<a href="#bookmark267">Khan and Heisterkamp,2016</a>, <a href="#bookmark319">Zadrozny, 2004</a>, <a href="#bookmark242">Cortes et al.,2008</a>, <a href="#bookmark243">Dai et al., 2007</a>, <a href="#bookmark302">Tan et al.,2015</a>, <a href="#bookmark303">Tan et al., 2017</a>] 着眼于对源域和目标域的分布比值进行估计(P(<strong>X</strong>t)/P(<strong>X</strong>s))。所估计得到的比值即为样本的权重。这些方法通常都假设P(<strong>x</strong>s) &lt;并且源域和目标域的条件概率分布相同(P(y|x~s~)=<em>P</em>(y|x~t~))。特别地，上海交通大学Dai等人[<a href="#bookmark243">Dai et al.,2007</a>]提出了 TrAdaboost方法，将AdaBoost的思想应用于迁移学习中，提高有利于目标分类任务的实例权重、降低不利于目标分类任务的实例权重，并基于PAC理论推导了模型的泛化误差上界。TrAdaBoost方法是此方面的经典研究之一。文献 [<a href="#bookmark264">Huang et al., 2007</a>]提出核均值匹配方法 (Kernel Mean atching, KMM)对于概率分布进行估计，目标是使得加权后的源域和目标域的概率分布尽可能相近。在最新的研究成果中，香港科技大学的Tan等人扩展了实例迁移学习方法的应用场景，提出 了传递迁移学习方法(Transitive Transfer Learning, TTL) [<a href="#bookmark302">Tan etal., 2015</a>] 和远域迁移学习 (Distant Domain Transfer Learning,DDTL) [<a href="#bookmark303">Tan et al., 2017</a>]，利用联合矩阵分解和深度神经网络，将迁移学习应用于多个不相似的领域之间的知识共享，取得了良好的效果。</p>
<p>​	虽然实例权重法具有较好的理论支撑、容易推导泛化误差上界，但这类方法通常只在领域间分布差异较小时有效，因此对自然语言处理、计算机视觉等任务效果并不理想。而基于特征表示的迁移学习方法效果更好,是我们研究的重点。</p>
<h3 id="11-2-2-基于特征迁移">11.2.2 基于特征迁移</h3>
<p>​	基于特征的迁移方法 (Feature based Transfer Learning) 是指将通过特征变换的方式互相迁移 [<a href="#bookmark272">Liu et al., 2011</a>, <a href="#bookmark327">Zheng et al.,2008</a>, <a href="#bookmark263">Hu and Yang, 2011</a>],来减少源域和目标域之间的差距；或者将源域和目标域的数据特征变换到统一特征空间中 [<a href="#bookmark288">Pan et al.,2011</a>, <a href="#bookmark278">Long et al., 2014b</a>, <a href="#bookmark248">Duan et al.,2012</a>],然后利用传统的机器学习方法进行分类识别。根据特征的同构和异构性,又可以分为同构和异构迁移学习。图<a href="#bookmark93">15</a>很形象地表示了两种基于特 征的迁移学习方法。</p>
<p><img src="media/fa08900e89bfd53cc28345d21bc6aca0.jpg" alt=""></p>
<center>图 15: 基于特征的迁移学习方法示意图
<p>​	基于特征的迁移学习方法是迁移学习领域中最热门的研究方法,这类方法通常假设源域和目标域间有一些交叉的特征。香港科技大学的 Pan 等人 [<a href="#bookmark288">Pan et al.,2011</a>] 提出的迁移 成分分析方法 (Transfer Component Analysis, TCA)是其中较为典型的一个方法。该方法的 核心内容是以最大均值差异 (Maximum MeanDiscrepancy, MMD) [<a href="#bookmark236">Borgwardt et al., 2006</a>]作为度量准则,将不同数据领域中的分布差异最小化。加州大学伯克利分校的 Blitzer 等人 [<a href="#bookmark235">Blitzer et al., 2006</a>] 提出了一种基于结构对应的学习方法(Structural Corresponding Learning,SCL),该算法可以通过映射将一个空间中独有的一些特征变换到其他所有空间中的轴特征上,然后在该特征上使用机器学习的算法进行分类预测。清华大学龙明盛等人[<a href="#bookmark278">Long et al.,2014b</a>]提出在最小化分布距离的同时，加入实例选择的迁移联合匹配(Tran-fer Joint Matching, TJM) 方法,将实例和特征迁移学习方法进行了有机的结合。澳大利亚卧龙岗大学的 Jing Zhang 等人 [<a href="#bookmark321">Zhang et al., 2017a</a>]提出对于源域和目标域各自训练不同 的变换矩阵,从而达到迁移学习的目标。</p>
<h3 id="11-2-3-基于模型迁移">11.2.3 基于模型迁移</h3>
<p>​	基于模型的迁移方法 (Parameter/Model based Transfer Learning) 是指从源域和目标域中找到他们之间共享的参数信息,以实现迁移的方法。这种迁移方式要求的假设条件是： 源域中的数据与目标域中的数据可以共享一些模型的参数。其中的代表性工作主要有［<a href="#bookmark324">Zhao et al., 2010</a>, <a href="#bookmark325">Zhao et al., 2011</a>, <a href="#bookmark287">Panet al., 2008b</a>, <a href="#bookmark286">Pan et al., 2008a</a>］。图<a href="#bookmark96">16</a>形象地 表示了基于模型的迁移学习方法的基本思想。</p>
<p><img src="media/602723a1d3ce0f3abe7c591a8e4bb6ec.jpg" alt=""></p>
<center>图 16: 基于模型的迁移学习方法示意图
<p>​	其中，中科院计算所的Zhao等人[<a href="#bookmark325">Zhao et al., 2011</a>]提出了TransEMDT方法。该方法首先针对已有标记的数据，利用决策树构建鲁棒性的行为识别模型，然后针对无标定数据，利用K-Means聚类方法寻找最优化的标定参数。西安邮电大学的Deng等人[<a href="#bookmark245">Deng et al.,2014</a>] 也用超限学习机做了类似的工作。香港科技大学的Pan等人[<a href="#bookmark286">Pan etal., 2008a</a>]利用HMM，针对Wifi室内定位在不同设备、不同时间和不同空间下动态变化的特点，进行不同分布下的室内定位研究。另一部分研究人员对支持向量机 SVM 进行了改进研究 [<a href="#bookmark285">Nater et al.,2011</a>, <a href="#bookmark269">Li et al., 2012</a>]。这些方法假定 SVM中的权重向量 <strong>w</strong> 可以分成两个部分： <strong>w</strong> = <strong>wo</strong>+<strong>v</strong>， 其中 <strong>w</strong>0代表源域和目标域的共享部分， <strong>v</strong> 代表了对于不同领域的特定处理。在最新的研究成果中，香港科技大学的 Wei 等人 [<a href="#bookmark313">Wei et al., 2016b</a>]将社交信息加入迁移学习方法的 正则项中，对方法进行了改进。清华大学龙明盛等人[<a href="#bookmark275">Long et al., 2015a</a>, <a href="#bookmark276">Long et al., 2016</a>, <a href="#bookmark280">Long etal., 2017</a>]改进了深度网络结构，通过在网络中加入概率分布适配层，进一步提高了深度迁移学习网络对于大数据的泛化能力。</p>
<h3 id="11-2-4-基于关系迁移">11.2.4 基于关系迁移</h3>
<p>​	基于关系的迁移学习方法 (Relation Based Transfer Learning) 与上述三种方法具有截然不同的思路。这种方法比较关注源域和目标域的样本之间的关系。图<a href="#bookmark82">17</a>形象地表示了不 同领域之间相似的关系。</p>
<p>​	就目前来说，基于关系的迁移学习方法的相关研究工作非常少，仅有几篇连贯式的文章讨论： [<a href="#bookmark283">Mihalkova et al., 2007</a>, <a href="#bookmark284">Mihalkova and Mooney,2008</a>, <a href="#bookmark244">Davis and Domingos, 2009</a>]。这些文章都借助于马尔科夫逻辑网络(Markov Logic Net)来挖掘不同领域之间的关系相似性。</p>
<p>​	我们将重点讨论基于特征和基于模型的迁移学习方法，这也是目前绝大多数研究工作的热点。</p>
<p><img src="media/aa10d36f758430dd4ff72d2bf6a76a6c.jpg" alt=""></p>
<center>图 17: 基于关系的迁移学习方法示意图
<p><img src="media/1542812440636.png" alt="1542812440636"></p>
<center>图 18: 基于马尔科夫逻辑网的关系迁移
<h2 id="11-3-迁移学习的常用方法">11.3 迁移学习的常用方法</h2>
<h3 id="11-3-1-数据分布自适应">11.3.1 数据分布自适应</h3>
<p>​	数据分布自适应 (Distribution Adaptation) 是一类最常用的迁移学习方法。这种方法的基本思想是,由于源域和目标域的数据概率分布不同,那么最直接的方式就是通过一些变换,将不同的数据分布的距离拉近。</p>
<p>​	图 <a href="#bookmark84">19</a>形象地表示了几种数据分布的情况。简单来说，数据的边缘分布不同，就是数据整体不相似。数据的条件分布不同，就是数据整体相似，但是具体到每个类里，都不太相似。</p>
<p><img src="media/1542812748062.png" alt="1542812748062"></p>
<center>图 19: 不同数据分布的目标域数据
<p>​	根据数据分布的性质,这类方法又可以分为边缘分布自适应、条件分布自适应、以及联合分布自适应。下面我们分别介绍每类方法的基本原理和代表性研究工作。介绍每类研究工作时,我们首先给出基本思路,然后介绍该类方法的核心,最后结合最近的相关工作介绍该类方法的扩展。</p>
<h3 id="11-3-2-边缘分布自适应">11.3.2 边缘分布自适应</h3>
<p>​	边缘分布自适应方法 (Marginal Distribution Adaptation) 的目标是减小源域和目标域的边缘概率分布的距离,从而完成迁移学习。从形式上来说,边缘分布自适应方法是用P(X~s~)和 P(X~t~)之间的距离来近似两个领域之间的差异。即：</p>
<p>​	<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>I</mi><mi>S</mi><mi>T</mi><mi>A</mi><mi>N</mi><mi>C</mi><mi>E</mi><mo stretchy="false">(</mo><mi>D</mi><mtext> </mtext><mi>s</mi><mtext> </mtext><mo separator="true">,</mo><mi>D</mi><mtext> </mtext><mi>t</mi><mtext> </mtext><mo stretchy="false">)</mo><mo>≈</mo><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi></mrow><annotation encoding="application/x-tex">DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.13889em;">ST</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05764em;">NCE</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace nobreak"> </span><span class="mord mathnormal">s</span><span class="mspace nobreak"> </span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace nobreak"> </span><span class="mord mathnormal">t</span><span class="mspace nobreak"> </span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∥</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∥</span></span></span></span> (6.1)</p>
<p>​	边缘分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark83">19(b)</a>的情形。</p>
<h3 id="11-3-3-条件分布自适应">11.3.3 条件分布自适应</h3>
<p>​	条件分布自适应方法 (Conditional Distribution Adaptation) 的目标是减小源域和目标域的条件概率分布的距离，从而完成迁移学习。从形式上来说，条件分布自适应方法是用  P(y~s~|X~s~) 和 P (y~t~|X~t~) 之间的距离来近似两个领域之间的差异。即：</p>
<p>​	<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>I</mi><mi>S</mi><mi>T</mi><mi>A</mi><mi>N</mi><mi>C</mi><mi>E</mi><mo stretchy="false">(</mo><mi>D</mi><mtext> </mtext><mi>s</mi><mtext> </mtext><mo separator="true">,</mo><mi>D</mi><mtext> </mtext><mi>t</mi><mtext> </mtext><mo stretchy="false">)</mo><mo>≈</mo><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>s</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi></mrow><annotation encoding="application/x-tex">DISTANCE(D~s~,D~t~)\approx\lVert P(y_s|X_s)-P(y_t|X_t)\Vert</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.13889em;">ST</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05764em;">NCE</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace nobreak"> </span><span class="mord mathnormal">s</span><span class="mspace nobreak"> </span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace nobreak"> </span><span class="mord mathnormal">t</span><span class="mspace nobreak"> </span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∥</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∥</span></span></span></span>(6.8)</p>
<p>​	条件分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark85">19©</a>的情形。</p>
<p>​	目前单独利用条件分布自适应的工作较少，这些工作主要可以在 [<a href="#bookmark292">Saito et al.,2017</a>] 中找到。最近，中科院计算所的 Wang 等人提出了 STL 方法(Stratified Transfer Learn­ing) [<a href="#bookmark309">Wang tal.,2018</a>]。作者提出了类内迁移 (Intra-class Transfer)的思想。指出现有的 绝大多数方法都只是学习一个全局的特征变换(Global DomainShift)，而忽略了类内的相 似性。类内迁移可以利用类内特征，实现更好的迁移效果。</p>
<p>​	STL 方法的基本思路如图所示。首先利用大多数投票的思想，对无标定的位置行为生成伪标；然后在再生核希尔伯特空间中，利用类内相关性进行自适应地空间降维，使得不同情境中的行为数据之间的相关性增大；最后，通过二次标定，实现对未知标定数据的精准标定。</p>
<p><img src="media/1542817481582.png" alt="1542817481582"></p>
<center>图 21: STL 方法的示意图
### 11.3.4 联合分布自适应
<p>​	联合分布自适应方法 (Joint Distribution Adaptation) 的目标是减小源域和目标域的联合概率分布的距离，从而完成迁移学习。从形式上来说，联合分布自适应方法是用<em>P</em>(<strong>x</strong>s) 和P(<strong>x</strong>t)之间的距离、以及P(ys|<strong>x</strong>s)和P(yt|<strong>x</strong>t)之间的距离来近似两个领域之间的差异。即:</p>
<p>​	<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>I</mi><mi>S</mi><mi>T</mi><mi>A</mi><mi>N</mi><mi>C</mi><mi>E</mi><mo stretchy="false">(</mo><mi>D</mi><mtext> </mtext><mi>s</mi><mtext> </mtext><mo separator="true">,</mo><mi>D</mi><mtext> </mtext><mi>t</mi><mtext> </mtext><mo stretchy="false">)</mo><mo>≈</mo><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi><mo>−</mo><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>s</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>s</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">∥</mi><mtext>​</mtext></mrow><annotation encoding="application/x-tex">DISTANCE(D~s~,D~t~)\approx\lVert P(X_s)-P(X_t)\Vert-\lVert P(y_s|X_s)-P(y_t|X_t)\Vert​</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal" style="margin-right:0.13889em;">ST</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.05764em;">NCE</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace nobreak"> </span><span class="mord mathnormal">s</span><span class="mspace nobreak"> </span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace nobreak"> </span><span class="mord mathnormal">t</span><span class="mspace nobreak"> </span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∥</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∥</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">∥</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∥​</span></span></span></span>(6.10)</p>
<p>​	联合分布自适应对应于图<a href="#bookmark84">19</a>中由图<a href="#bookmark101">19(a)</a>迁移到图<a href="#bookmark83">19(b)</a>的情形、以及图<a href="#bookmark101">19(a)</a>迁移到<br>
图<a href="#bookmark85">19©</a>的情形。</p>
<h3 id="11-3-4-概率分布自适应方法优劣性比较">11.3.4 概率分布自适应方法优劣性比较</h3>
<p>综合上述三种概率分布自适应方法，我们可以得出如下的结论：</p>
<ol>
<li>精度比较： BDA &gt;JDA &gt;TCA &gt;条件分布自适应。</li>
<li>将不同的概率分布自适应方法用于神经网络，是一个发展趋势。图<a href="#bookmark119">23</a>展示的结果表明将概率分布适配加入深度网络中，往往会取得比非深度方法更好的结果。</li>
</ol>
<p><img src="media/1542823019007.png" alt="1542823019007"></p>
<center>图 22: BDA 方法的效果第二类方法：特征选择
### 11.3.6 特征选择
<p>​	特征选择法的基本假设是：源域和目标域中均含有一部分公共的特征，在这部分公共的特征，源领域和目标领域的数据分布是一致的。因此，此类方法的目标就是，通过机器学习方法，选择出这部分共享的特征，即可依据这些特征构建模型。</p>
<p>​	图 <a href="#bookmark122">24</a>形象地表示了特征选择法的主要思路。</p>
<p><img src="media/1542823210556.png" alt="1542823210556"></p>
<center>图 23: 不同分布自适应方法的精度比较
<p><img src="media/a3db84158d9b6454adff88dbe4fa5d28.jpg" alt=""></p>
<center>图 24: 特征选择法示意图
<p>​	这这个领域比较经典的一个方法是发表在 2006 年的 ECML-PKDD 会议上,作者提出了一个叫做 SCL 的方法 (Structural Correspondence Learning) [<a href="#bookmark235">Blitzer et al.,2006</a>]。这个方法的目标就是我们说的,找到两个领域公共的那些特征。作者将这些公共的特征叫做Pivot feature。找出来这些Pivot feature,就完成了迁移学习的任务。</p>
<p><img src="media/4abacd82901988c3e0a98bdb07b2abc6.jpg" alt=""></p>
<center>图 25: 特征选择法中的 Pivot feature 示意图
<p>​	图 <a href="#bookmark124">25</a>形象地展示了 Pivot feature 的含义。 Pivot feature指的是在文本分类中,在不同领域中出现频次较高的那些词。总结起来：</p>
<ul>
<li>特征选择法从源域和目标域中选择提取共享的特征,建立统一模型</li>
<li>通常与分布自适应方法进行结合</li>
<li>通常采用稀疏表示 ||<strong>A</strong>||2,1 实现特征选择</li>
</ul>
<h3 id="11-3-5-统计特征对齐方法">11.3.5 统计特征对齐方法</h3>
<p>​	统计特征对齐方法主要将数据的统计特征进行变换对齐。对齐后的数据，可以利用传统机器学习方法构建分类器进行学习。SA方法(Subspace Alignment，子空间对齐)[<a href="#bookmark249">Fernando et al.,2013</a>]是其中的代表性成果。SA方法直接寻求一个线性变换<strong>M</strong>，将不同的数据实现变换对齐。SA方法的优化目标如下：</p>
<p><img src="media/1542823438846.png" alt="1542823438846"></p>
<p>则变换 <strong>M</strong> 的值为：</p>
<p><img src="media/1542823455820.png" alt="1542823455820"></p>
<p>可以直接获得上述优化问题的闭式解：</p>
<p><img src="media/1542823474720.png" alt="1542823474720"></p>
<p>​	SA 方法实现简单，计算过程高效，是子空间学习的代表性方法。</p>
<h3 id="11-3-6-流形学习方法">11.3.6 流形学习方法</h3>
<p><strong>什么是流形学习</strong></p>
<p>​	流形学习自从 2000 年在 Science 上被提出来以后,就成为了机器学习和数据挖掘领域的热门问题。它的基本假设是,现有的数据是从一个高维空间中采样出来的,所以,它具有高维空间中的低维流形结构。流形就是是一种几何对象（就是我们能想像能观测到的）。通俗点说就是,我们无法从原始的数据表达形式明显看出数据所具有的结构特征,那我把它想像成是处在一个高维空间,在这个高维空间里它是有个形状的。一个很好的例子就是星座。满天星星怎么描述？我们想像它们在一个更高维的宇宙空间里是有形状的,这就有了各自星座,比如织女座、猎户座。流形学习的经典方法有Isomap、locally linear embedding、 laplacian eigenmap 等。</p>
<p>​	流形空间中的距离度量：两点之间什么最短？在二维上是直线（线段）,可在三维呢？地球上的两个点的最短距离可不是直线,它是把地球展开成二维平面后画的那条直线。那条线在三维的地球上就是一条曲线。这条曲线就表示了两个点之间的最短距离,我们叫它测地线。更通俗一点, 两点之间，测地线最短。在流形学习中,我们遇到测量距离的时候更多的时候用的就是这个测地线。在我们要介绍的 GFK 方法中,也是利用了这个测地线距离。比如在下面的图中,从 A 到 C 最短的距离在就是展开后的线段,但是在三维球体上看它却是一条曲线。</p>
<p><img src="media/fcbe02803e45f6455a4602b645b472c5.jpg" alt=""></p>
<center>图 28: 三维空间中两点之间的距离示意图
<p>​	由于在流形空间中的特征通常都有着很好的几何性质,可以避免特征扭曲,因此我们首先将原始空间下的特征变换到流形空间中。在众多已知的流形中, Grassmann 流形G（d） 可以通过将原始的 d 维子空间 （特征向量）看作它基础的元素,从而可以帮助学习分类 器。在 Grassmann流形中,特征变换和分布适配通常都有着有效的数值形式,因此在迁移学习问题中可以被很高效地表示和求解 [<a href="#bookmark260">Hamm and Lee,2008</a>]。因此,利用 Grassmann流形空间中来进行迁移学习是可行的。现存有很多方法可以将原始特征变换到流形空间 中[<a href="#bookmark257">Gopalan et al., 2011</a>, <a href="#bookmark230">Baktashmotlagh et al.,2014</a>]。</p>
<p>​	在众多的基于流形变换的迁移学习方法中，GFK(Geodesic Flow Kernel)方法[<a href="#bookmark255">Gong et<br>
al., 2012</a>]是最为代表性的一个。GFK是在2011年发表在ICCV上的SGF方法[<a href="#bookmark257">Gopalan et al.,<br>
2011</a>]发展起来的。我们首先介绍SGF方法。</p>
<p>​	SGF 方法从增量学习中得到启发：人类从一个点想到达另一个点，需要从这个点一步一步走到那一个点。那么，如果我们把源域和目标域都分别看成是高维空间中的两个点，由源域变换到目标域的过程不就完成了迁移学习吗？也就是说， 路是一步一步走出来的。</p>
<p>​	于是 SGF 就做了这个事情。它是怎么做的呢？把源域和目标域分别看成高维空间 (即Grassmann流形)中的两个点，在这两个点的测地线距离上取d个中间点，然后依次连接起来。这样，源域和目标域就构成了一条测地线的路径。我们只需要找到合适的每一步的变换，就能从源域变换到目标域了。图 <a href="#bookmark133">29</a>是 SGF 方法的示意图。</p>
<p><img src="media/103de3658cbb97ad4c24bafe28f9d957.jpg" alt=""></p>
<center>图 29: SGF 流形迁移学习方法示意图
<p>​	SGF 方法的主要贡献在于：提出了这种变换的计算及实现了相应的算法。但是它有很明显的缺点：到底需要找几个中间点？ SGF也没能给出答案，就是说这个参数d是没法估计的，没有一个好的方法。这个问题在 GFK 中被回答了。</p>
<p>​	GFK方法首先解决SGF的问题：如何确定中间点的个数d。它通过提出一种核学习的方法，利用路径上的无穷个点的积分，把这个问题解决了。这是第一个贡献。然后，它又解决了第二个问题：当有多个源域的时候，我们如何决定使用哪个源域跟目标域进行迁移？ GFK通过提出Rank of Domain度量，度量出跟目标域最近的源域，来解决这个问题。图 <a href="#bookmark134">30</a>是 GFK 方法的示意图。</p>
<p><img src="media/e654d14df0b44ee4e8a0e505c654044b.jpg" alt=""></p>
<center>图 30: GFK 流形迁移学习方法示意图
<p>​	用Ss和St分别表示源域和目标域经过主成分分析(PCA)之后的子空间，则G可以视为所有的d维子空间的集合。每一个d维的原始子空间都可以被看作G上的一个点。因此，在两点之间的测地线｛$(t) :0 &lt; t &lt;1｝可以在两个子空间之间构成一条路径。如果我 们令Ss = $(0)，St =$(1)，则寻找一条从$(0)到$(1)的测地线就等同于将原始的特征变换到一个无穷维度的空间中，最终减小域之间的漂移现象。这种方法可以被看作是一种从$(0)到$(1)的増量式“行走”方法。</p>
<p>​	特别地，流形空间中的特征可以被表示为<strong>z</strong> =$(t)T<strong>x</strong>。变换后的特征<strong>Z</strong>i和<strong>Z</strong>j的内积定义了一个半正定 (positive semidefinite) 的测地线流式核</p>
<p><img src="media/1542823895008.png" alt="1542823895008"></p>
<p>​	GFK 方法详细的计算过程可以参考原始的文章，我们在这里不再赘述。</p>
<h3 id="11-3-7-什么是finetune？">11.3.7 什么是finetune？</h3>
<p>​	深度网络的finetune也许是最简单的深度网络迁移方法。<strong>Finetune</strong>,也叫微调、fine-tuning, 是深度学习中的一个重要概念。简而言之，finetune就是利用别人己经训练好的网络，针对自己的任务再进行调整。从这个意思上看，我们不难理解finetune是迁移学习的一部分。</p>
<p><strong>为什么需要已经训练好的网络？</strong></p>
<p>​	在实际的应用中,我们通常不会针对一个新任务,就去从头开始训练一个神经网络。这样的操作显然是非常耗时的。尤其是，我们的训练数据不可能像ImageNet那么大，可以训练出泛化能力足够强的深度神经网络。即使有如此之多的训练数据,我们从头开始训练,其代价也是不可承受的。</p>
<p>​	那么怎么办呢？迁移学习告诉我们,利用之前己经训练好的模型,将它很好地迁移到自己的任务上即可。</p>
<p><strong>为什么需要 finetune？</strong></p>
<p>​	因为别人训练好的模型,可能并不是完全适用于我们自己的任务。可能别人的训练数据和我们的数据之间不服从同一个分布；可能别人的网络能做比我们的任务更多的事情；可能别人的网络比较复杂,我们的任务比较简单。</p>
<p>​	举一个例子来说,假如我们想训练一个猫狗图像二分类的神经网络,那么很有参考价值的就是在 CIFAR-100 上训练好的神经网络。但是 CIFAR-100 有 100 个类别,我们只需要 2个类别。此时,就需要针对我们自己的任务,固定原始网络的相关层,修改网络的输出层以使结果更符合我们的需要。</p>
<p>​	图<a href="#bookmark148">36</a>展示了一个简单的finetune过程。从图中我们可以看到，我们采用的预训练好的网络非常复杂,如果直接拿来从头开始训练,则时间成本会非常高昂。我们可以将此网络进行改造,固定前面若干层的参数,只针对我们的任务,微调后面若干层。这样,网络训练速度会极大地加快,而且对提高我们任务的表现也具有很大的促进作用。</p>
<p><img src="media/b1630ca5d004d4b430672c8b8ce7fb90.jpg" alt=""></p>
<center>图 36: 一个简单的 finetune 示意图
**Finetune 的优势**
<p>​	Finetune 的优势是显然的，包括:</p>
<ul>
<li>不需要针对新任务从头开始训练网络，节省了时间成本；</li>
<li>预训练好的模型通常都是在大数据集上进行的，无形中扩充了我们的训练数据，使得模型更鲁棒、泛化能力更好；</li>
<li>Finetune 实现简单，使得我们只关注自己的任务即可。</li>
</ul>
<p><strong>Finetune 的扩展</strong></p>
<p>​	在实际应用中，通常几乎没有人会针对自己的新任务从头开始训练一个神经网络。Fine-tune 是一个理想的选择。</p>
<p>​	Finetune 并不只是针对深度神经网络有促进作用，对传统的非深度学习也有很好的效果。例如， finetune对传统的人工提取特征方法就进行了很好的替代。我们可以使用深度网络对原始数据进行训练，依赖网络提取出更丰富更有表现力的特征。然后，将这些特征作为传统机器学习方法的输入。这样的好处是显然的: 既避免了繁复的手工特征提取，又能自动地提取出更有表现力的特征。</p>
<p>​	比如，图像领域的研究，一直是以 SIFT、SURF 等传统特征为依据的，直到 2014 年，伯克利的研究人员提出了 DeCAF特征提取方法［<a href="#bookmark246">Donahue et al.,2014</a>］，直接使用深度卷积神经网络进行特征提取。实验结果表明，该特征提取方法对比传统的图像特征，在精度上有着无可匹敌的优势。另外，也有研究人员用卷积神经网络提取的特征作为SVM分类器的输 入［<a href="#bookmark291">Razavian et al.,014</a>］，显著提升了图像分类的精度。</p>
<h3 id="11-3-8-finetune为什么有效？">11.3.8 finetune为什么有效？</h3>
<p>​	随着 AlexNet [<a href="#bookmark268">Krizhevsky et al., 2012</a>] 在 2012 年的 ImageNet大赛上获得冠军，深度学习开始在机器学习的研究和应用领域大放异彩。尽管取得了很好的结果，但是神经网络本身就像一个黑箱子，看得见，摸不着，解释性不好。由于神经网络具有良好的层次结构很自然地就有人开始关注，能否通过这些层次结构来很好地解释网络？于是，有了我们熟知的例子：假设一个网络要识别一只猫，那么一开始它只能检测到一些边边角角的东西，和猫根本没有关系；然后可能会检测到一些线条和圆形；慢慢地，可以检测到有猫的区域；接着是猫腿、猫脸等等。图 <a href="#bookmark137">32</a>是一个简单的示例。</p>
<p><img src="media/1542824195602.png" alt="1542824195602"></p>
<center>图 32: 深度神经网络进行特征提取到分类的简单示例
<p>​	这表达了一个什么事实呢？概括来说就是：前面几层都学习到的是通用的特征（general feature）；随着网络层次的加深，后面的网络更偏重于学习任务特定的特征（specific feature）。<br>
这非常好理解，我们也都很好接受。那么问题来了：如何得知哪些层能够学习到 general feature，哪些层能够学习到specific feature。更进一步：如果应用于迁移学习，如何决定该迁移哪些层、固定哪些层？</p>
<p>​	这个问题对于理解神经网络以及深度迁移学习都有着非常重要的意义。</p>
<p>​	来自康奈尔大学的 Jason Yosinski 等人 [<a href="#bookmark318">Yosinski et al., 2014</a>]率先进行了深度神经网络可迁移性的研究，将成果发表在2014年机器学习领域顶级会议NIPS上并做了口头汇报。该论文是一篇实验性质的文章（通篇没有一个公式）。其目的就是要探究上面我们提到的几个关键性问题。因此，文章的全部贡献都来自于实验及其结果。（别说为啥做实验也能发文章：都是高考，我只上了个普通一本，我高中同学就上了清华）</p>
<p>​	在ImageNet的1000类上，作者把1000类分成两份（A和B），每份500个类别。然后，分别对A和B基于Caffe训练了一个AlexNet网络。一个AlexNet网络一共有8层， 除去第8层是类别相关的网络无法迁移以外，作者在 1 到 7这 7层上逐层进行 finetune 实验，探索网络的可迁移性。</p>
<p>​	为了更好地说明 finetune 的结果，作者提出了有趣的概念： AnB 和 BnB。</p>
<p>​	迁移A网络的前n层到B （AnB） vs固定B网络的前n层（BnB）</p>
<p>​	简单说一下什么叫AnB:（所有实验都是针对数据B来说的）将A网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对B进行分类。</p>
<p>​	相应地，有BnB:把训练好的B网络的前n层拿来并将它frozen，剩下的8 - n层随机初始化，然后对 B 进行分类。</p>
<p>​	<strong>实验结果</strong></p>
<p>​	实验结果如下图（图<a href="#bookmark145">33</a>） 所示:</p>
<p>​	这个图说明了什么呢？我们先看蓝色的BnB和BnB+（就是BnB加上finetune）。对 BnB而言，原训练好的 B 模型的前 3 层直接拿来就可以用而不会对模型精度有什么损失到了第4 和第5 层，精度略有下降，不过还是可以接受。然而到了第6 第第7层，精度居然奇迹般地回升了！这是为什么？原因如下:对于一开始精度下降的第4 第 5 层来说，确</p>
<p><img src="media/1542824318155.png" alt="1542824318155"></p>
<center>图 33: 深度网络迁移实验结果 1
<p>实是到了这一步，feature变得越来越specific,所以下降了。那对于第6第7层为什么精度又不变了？那是因为，整个网络就8层，我们固定了第6第7层，这个网络还能学什么呢？所以很自然地，精度和原来的 B 网络几乎一致！</p>
<p>​	对 BnB+ 来说，结果基本上都保持不变。说明 finetune 对模型结果有着很好的促进作用！</p>
<p>​	我们重点关注AnB和AnB+。对AnB来说，直接将A网络的前3层迁移到B,貌似不会有什么影响，再一次说明，网络的前3层学到的几乎都是general feature!往后，到了第4第5层的时候，精度开始下降，我们直接说：一定是feature不general 了！然而，到了第6第7层，精度出现了小小的提升后又下降，这又是为什么？作者在这里提出两点co-adaptation和feature representation。就是说，第4第5层精度下降的时候，主要是由于A和B两个数据集的差异比较大，所以会下降；至I」了第6第7层，由于网络几乎不迭代了，学习能力太差，此时 feature 学不到，所以精度下降得更厉害。</p>
<p>​	再看AnB+。加入了 finetune以后，AnB+的表现对于所有的n几乎都非常好，甚至 比baseB<br>
（最初的B）还要好一些！这说明：finetune对于深度迁移有着非常好的促进作用!</p>
<p>​	把上面的结果合并就得到了下面一张图 （图<a href="#bookmark138">34</a>）：</p>
<p>​	至此， AnB 和 BnB 基本完成。作者又想，是不是我分 A 和 B 数据的时候，里面存在一些比较相似的类使结果好了？比如说A里有猫，B里有狮子，所以结果会好？为了排除这些影响，作者又分了一下数据集，这次使得A和B里几乎没有相似的类别。在这个条件下再做AnB,与原来精度比较（0%为基准）得到了下图（图<a href="#bookmark139">35</a>）:</p>
<p>​	这个图说明了什么呢？简单：随着可迁移层数的增加，模型性能下降。但是，前3层仍然还是可以迁移的！同时,与随机初始化所有权重比较,迁移学习的精度是很高的!总之：</p>
<ul>
<li>
<p>深度迁移网络要比随机初始化权重效果好；</p>
</li>
<li>
<p>网络层数的迁移可以加速网络的学习和优化。</p>
</li>
</ul>
<h3 id="11-3-9-什么是深度网络自适应？">11.3.9 什么是深度网络自适应？</h3>
<p><strong>基本思路</strong></p>
<p>​	深度网络的 finetune 可以帮助我们节省训练时间，提高学习精度。但是 finetune 有它的先天不足:它无法处理训练数据和测试数据分布不同的情况。而这一现象在实际应用中比比皆是。因为 finetune 的基本假设也是训练数据和测试数据服从相同的数据分布。这在迁移学习中也是不成立的。因此，我们需要更进一步，针对深度网络开发出更好的方法使之更好地完成迁移学习任务。</p>
<p>​	以我们之前介绍过的数据分布自适应方法为参考，许多深度学习方法［<a href="#bookmark307">Tzeng et al.,2014</a>, <a href="#bookmark275">Long et al.,2015a</a>］都开发出了自适应层(AdaptationLayer)来完成源域和目标域数据的自适应。自适应能够使得源域和目标域的数据分布更加接近，从而使得网络的效果更好。</p>
<p>​	从上述的分析我们可以得出，深度网络的自适应主要完成两部分的工作:</p>
<p>​	一是哪些层可以自适应，这决定了网络的学习程度；</p>
<p>​	二是采用什么样的自适应方法 (度量准则)，这决定了网络的泛化能力。</p>
<p>​	深度网络中最重要的是网络损失的定义。绝大多数深度迁移学习方法都采用了以下的损失定义方式:</p>
<p><img src="media/1542824918145.png" alt="1542824918145"></p>
<p>​	其中，I表示网络的最终损失，lc(Ds,<strong>y</strong>s)表示网络在有标注的数据(大部分是源域)上的常规分类损失(这与普通的深度网络完全一致)，Ia(Ds,Dt)表示网络的自适应损失。最后一部分是传统的深度网络所不具有的、迁移学习所独有的。此部分的表达与我们先前讨论过的源域和目标域的分布差异，在道理上是相同的。式中的A是权衡两部分的权重参数。</p>
<p>​	上述的分析指导我们设计深度迁移网络的基本准则：决定自适应层，然后在这些层加入自适应度量，最后对网络进行 finetune。</p>
<h3 id="11-3-10-GAN在迁移学习中的应用">11.3.10 GAN在迁移学习中的应用</h3>
<p>生成对抗网络 GAN(Generative Adversarial Nets) [<a href="#bookmark256">Goodfellow et al.,2014</a>] 是目前人工智能领域最炙手可热的概念之一。其也被深度学习领军人物 Yann Lecun 评为近年来最令人欣喜的成就。由此发展而来的对抗网络，也成为了提升网络性能的利器。本小节介绍深度对抗网络用于解决迁移学习问题方面的基本思路以及代表性研究成果。</p>
<p><strong>基本思路</strong></p>
<p>​	GAN 受到自博弈论中的二人零和博弈 (two-player game) 思想的启发而提出。它一共包括两个部分：一部分为生成网络(Generative Network)，此部分负责生成尽可能地以假乱真的样本，这部分被成为生成器(Generator)；另一部分为判别网络(Discriminative Network), 此部分负责判断样本是真实的，还是由生成器生成的，这部分被成为判别器(Discriminator) 生成器和判别器的互相博弈，就完成了对抗训练。</p>
<p>​	GAN 的目标很明确：生成训练样本。这似乎与迁移学习的大目标有些许出入。然而，由于在迁移学习中，天然地存在一个源领域，一个目标领域，因此，我们可以免去生成样本的过程，而直接将其中一个领域的数据 (通常是目标域) 当作是生成的样本。此时，生成器的职能发生变化，不再生成新样本，而是扮演了特征提取的功能：不断学习领域数据的特征使得判别器无法对两个领域进行分辨。这样，原来的生成器也可以称为特征提取器<br>
(Feature Extractor)。</p>
<p>​	通常用 Gf 来表示特征提取器，用 Gd 来表示判别器。正是基于这样的领域对抗的思想，深度对抗网络可以被很好地运用于迁移学习问题中。与深度网络自适应迁移方法类似，深度对抗网络的损失也由两部分构成：网络训练的损失lc*和领域判别损失Id：</p>
<p><img src="media/1542826334834.png" alt="1542826334834"></p>
<p><strong>DANN</strong></p>
<p>Yaroslav Ganin 等人 [<a href="#bookmark251">Ganin et al., 2016</a>]首先在神经网络的训练中加入了对抗机制，作者将他们的网络称之为DANN(Domain-Adversarial Neural Network)。在此研宄中，网络的学习目标是：生成的特征尽可能帮助区分两个领域的特征，同时使得判别器无法对两个领域的差异进行判别。该方法的领域对抗损失函数表示为：</p>
<p><img src="media/1542826461988.png" alt="1542826461988"></p>
<p>Id = max 其中的 Ld 表示为</p>
<p><img src="media/1542826475517.png" alt="1542826475517"></p>
<h2 id="参考文献-9">参考文献</h2>
<p>王晋东，迁移学习简明手册</p>
<p>[Baktashmotlagh et al., 2013] Baktashmotlagh, M., Harandi, M. T., Lovell, B. C.,and Salz- mann, M. (2013). Unsupervised domain adaptation by domain invariant projection. In <em>ICCV,</em> pages 769-776.</p>
<p>[Baktashmotlagh et al., 2014] Baktashmotlagh, M., Harandi, M. T., Lovell, B. C., and Salz- mann, M. (2014). Domain adaptation on the statistical manifold. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,pages 2481-2488.</p>
<p>[Ben-David et al., 2010] Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. (2010). A theory of learning from different domains. <em>Machine learning,</em> 79(1-2):151-175.</p>
<p>[Ben-David et al., 2007] Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. (2007). Analysis of representations for domain adaptation. In <em>NIPS</em>, pages 137-144.</p>
<p>[Blitzer et al., 2008] Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wortman, J. (2008). Learning bounds for domain adaptation. In <em>Advances in neural information processing systems</em>, pages 129-136.</p>
<p>[Blitzer et al., 2006] Blitzer, J., McDonald, R., and Pereira, F. (2006). Domain adaptation with structural correspondence learning. In <em>Proceedings of the 2006 conference on empiri­cal methods in natural language processing</em>, pages 120-128. Association for Computational Linguistics.</p>
<p>[Borgwardt et al., 2006] Borgwardt, K. M., Gretton, A., Rasch, M. J., Kriegel, H.-P., Scholkopf, B., and Smola, A. J. (2006). Integrating structured biological data by kernel maximum mean discrepancy. <em>Bioinformatics</em>, 22(14):e49-e57.</p>
<p>[Bousmalis et al., 2016] Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D. (2016). Domain separation networks. In <em>Advances in Neural Information Processing Systems</em>, pages 343-351.</p>
<p>[Cai et al., 2011] Cai, D., He, X., Han, J., and Huang, T. S. (2011). Graph regularized nonnegative matrix factorization for data representation. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 33(8):1548-1560.</p>
<p>[Cao et al., 2017] Cao, Z., Long, M., Wang, J., and Jordan, M. I. (2017). Partial transfer learning with selective adversarial networks. <em>arXiv preprint arXiv:1707.07901</em>.</p>
<p>[Carlucci et al., 2017] Carlucci, F. M., Porzi, L., Caputo, B., Ricci, E., and Bulo, S. R. (2017). Autodial: Automatic domain alignment layers. In International Conference on* Computer Vision.</p>
<p>[Cook et al., 2013] Cook, D., Feuz, K. D., and Krishnan, N. C. (2013). Transfer learning for activity recognition: A survey. <em>Knowledge and information systems</em>, 36(3):537-556.</p>
<p>[Cortes et al., 2008] Cortes, C., Mohri, M., Riley, M., and Rostamizadeh, A. (2008). Sample selection bias correction theory. In <em>International Conference on Algorithmic Learning Theory</em>, pages 38-53, Budapest, Hungary. Springer.</p>
<p>[Dai et al., 2007] Dai, W., Yang, Q., Xue, G.-R., and Yu, Y. (2007). Boosting for transfer learning. In <em>ICML</em>, pages 193-200. ACM.</p>
<p>[Davis and Domingos, 2009] Davis, J. and Domingos, P. (2009). Deep transfer via second- order markov logic. In <em>Proceedings of the 26th annual international conference on machine learning</em>, pages 217-224. ACM.</p>
<p>[Denget al., 2014] Deng,W.,Zheng,Q.,andWang,Z.(2014).Cross-personactivityrecog-nition using reduced kernel extreme learning machine. <em>Neural Networks,</em> 53:1-7.</p>
<p>[Donahue et al., 2014] Donahue, J., Jia, Y., et al. (2014). Decaf: A deep convolutional activation feature for generic visual recognition. In <em>ICML</em>, pages 647-655.</p>
<p>[Dorri and Ghodsi, 2012] Dorri, F. and Ghodsi, A. (2012). Adapting component analysis. In <em>Data Mining (ICDM), 2012 IEEE 12th International Conference on</em>, pages 846-851. IEEE.</p>
<p>[Duan et al., 2012] Duan, L., Tsang, I. W., and Xu, D. (2012). Domain transfer multi­ple kernel learning. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 34(3):465-479.</p>
<p>[Fernando et al., 2013] Fernando, B., Habrard, A., Sebban, M., and Tuytelaars, T. (2013). Unsupervised visual domain adaptation using subspace alignment. In ICCV*, pages 2960­2967.</p>
<p>[Fodor, 2002] Fodor, I. K. (2002). A survey of dimension reduction techniques. Center for Applied Scientific Computing, Lawrence Livermore National Laboratory*, 9:1-18.</p>
<p>[Ganin et al., 2016] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Lavi- olette, F., Marchand, M., and Lempitsky, V. (2016).Domain-adversarial training of neural networks. <em>Journal of Machine Learning<br>
Research</em>, 17(59):1-35.</p>
<p>[Gao et al., 2012] Gao, C., Sang, N., and Huang, R. (2012). Online transfer boosting for object tracking. In <em>Pattern Recognition (ICPR), 2012 21st International Conference on</em>, pages 906-909. IEEE.</p>
<p>[Ghifary et al., 2017] Ghifary, M., Balduzzi, D., Kleijn, W. B., and Zhang, M. (2017). Scat­ter component analysis: A unified framework for domain adaptation and domain general­ization. <em>IEEE transactions on pattern analysis and machine intelligence</em>, 39(7):1414-1430.</p>
<p>[Ghifary et al., 2014] Ghifary, M., Kleijn, W. B., and Zhang, M. (2014). Domain adaptive neural networks for object recognition. In <em>PRICAI</em>, pages 898-904.</p>
<p>[Gong et al., 2012] Gong, B., Shi, Y., Sha, F., and Grauman, K. (2012). Geodesic flow kernel for unsupervised domain adaptation. In <em>CVPR</em>, pages 2066-2073.</p>
<p>[Goodfellow et al., 2014] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,  Warde- Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In <em>Advances in neural information processing systems</em>, pages 2672-2680.</p>
<p>[Gopalan et al., 2011] Gopalan, R., Li, R., and Chellappa, R. (2011). Domain adaptation for object recognition: An unsupervised approach. In <em>ICCV</em>, pages 999-1006. IEEE.</p>
<p>[Gretton et al., 2012] Gretton, A., Sejdinovic, D., Strathmann, H., Balakrishnan, S., Pontil, M., Fukumizu, K., and Sriperumbudur, B. K. (2012). Optimal kernel choice for large- scale two-sample tests. In <em>Advances in neural information processing systems</em>, pages 1205-1213.</p>
<p>[Gu et al., 2011] Gu, Q., Li, Z., Han, J., et al. (2011). Joint feature selection and subspace learning. In <em>IJCAI Proceedings-International Joint Conference on Artificial Intel ligence</em>, volume 22, page 1294.</p>
<p>[Hamm and Lee, 2008] Hamm, J. and Lee, D. D. (2008). Grassmann discriminant analysis: a unifying view on subspace-based learning. In <em>ICML</em>, pages 376-383. ACM.</p>
<p>[Hou et al., 2015] Hou, C.-A., Yeh, Y.-R., and Wang, Y.-C. F. (2015). An unsupervised domain adaptation approach for cross-domain visual classification. In <em>Advanced Video and Signal Based Surveil lance (AVSS), 2015 12th IEEE International Conference on</em>,pages 1-6. IEEE.</p>
<p>[Hsiao et al., 2016] Hsiao, P.-H., Chang, F.-J., and Lin, Y.-Y. (2016). Learning discrim­inatively reconstructed source data for object recognition with few examples. <em>IEEE</em>Transactions on Image Processing*, 25(8):3518-3532.</p>
<p>[Hu and Yang, 2011] Hu, D. H. and Yang, Q. (2011). Transfer learning for activity recog­nition via sensor mapping. In <em>IJCAI Proceedings-International Joint Conference on Artificial Intelligence</em>, volume 22, page 1962, Barcelona, Catalonia, Spain. IJCAI.</p>
<p>[Huang et al., 2007] Huang, J., Smola, A. J., Gretton, A., Borgwardt, K. M., Scholkopf, B., et al. (2007). Correcting sample selection bias by unlabeled  data. <em>Advances in neural information processing systems</em>, 19:601.</p>
<p>[Jaini et al., 2016] Jaini, P., Chen, Z., Carbajal, P., Law, E., Middleton, L., Regan, K., Schaekermann, M., Trimponias, G., Tung, J., and Poupart, P. (2016). Online bayesian transfer learning for sequential data modeling. In <em>ICLR 2017</em>.</p>
<p>[Kermany et al., 2018] Kermany, D. S., Goldbaum, M., Cai, W., Valentim, C. C., Liang, H., Baxter, S. L., McKeown, A., Yang, G., Wu, X., Yan, F., et al. (2018). Identifying medical diagnoses and treatable diseases by image-based deep learning. <em>Cell</em>, 172(5):1122-1131.</p>
<p>[Khan and Heisterkamp, 2016] Khan, M. N. A. and Heisterkamp, D. R. (2016). Adapting instance weights for unsupervised domain adaptation using quadratic mutual informa­tion and subspace learning. In <em>Pattern Recognition (ICPR), 2016 23rd International Conference on</em>, pages 1560-1565, Mexican City. IEEE.</p>
<p>[Krizhevsky et al., 2012] Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems*, pages 1097-1105.</p>
<p>[Li et al., 2012] Li, H., Shi, Y., Liu, Y., Hauptmann, A. G., and Xiong, Z. (2012). Cross­domain video concept detection: A joint discriminative and generative active learning approach. <em>Expert Systems with Applications</em>,<br>
39(15):12220-12228.</p>
<p>[Li et al., 2016] Li, J., Zhao, J., and Lu, K. (2016). Joint feature selection and structure preservation for domain adaptation. In <em>Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</em>, pages<br>
1697-1703. AAAI Press.</p>
<p>[Li et al., 2018] Li, Y., Wang, N., Shi, J., Hou, X., and Liu, J. (2018). Adaptive batch normalization for practical domain adaptation. <em>Pattern Recognition</em>, 80:109-117.</p>
<p>[Liu et al., 2011] Liu, J., Shah, M., Kuipers, B., and Savarese, S. (2011). Cross-view action recognition via view knowledge transfer. In <em>Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</em>, pages 3209-3216, Colorado Springs, CO, USA. IEEE.</p>
<p>[Liu and Tuzel, 2016] Liu, M.-Y. and Tuzel, O. (2016). Coupled generative adversarial networks. In <em>Advances in neural information processing systems</em>, pages 469-477.</p>
<p>[Liu et al., 2017] Liu, T., Yang, Q., and Tao, D. (2017). Understanding how  feature struc­ture transfers in transfer learning. In <em>IJCAI</em>.</p>
<p>[Long et al., 2015a] Long, M., Cao, Y., Wang, J., and Jordan, M. (2015a). Learning trans­ferable features with deep adaptation networks. In <em>ICML</em>, pages 97-105.</p>
<p>[Long et al., 2016] Long, M., Wang, J., Cao, Y., Sun, J., and Philip, S. Y. (2016). Deep learning of transferable representation for scalable domain  adaptation. <em>IEEE Transac­tions on Knowledge and Data Engineering</em>,<br>
28(8):2027-2040.</p>
<p>[Long et al., 2014a] Long, M., Wang, J., Ding, G., Pan, S. J., and Yu, P. S. (2014a). Adaptation regularization: A general framework for transfer learning.*IEEE TKDE, 26(5):1076-1089.</p>
<p>[Long et al., 2014b] Long, M., Wang, J., Ding, G., Sun, J., and Yu, P. S. (2014b). Transfer joint matching for unsupervised domain adaptation. In *CVPR ,pages 1410-1417.</p>
<p>[Long et al., 2013] Long, M., Wang, J., et al. (2013). Transfer feature learning with joint distribution adaptation. In <em>ICCV</em>, pages 2200-2207.</p>
<p>[Long et al., 2017] Long, M., Wang, J., and Jordan, M. I. (2017). Deep transfer learning with joint adaptation networks. In <em>ICML</em>, pages 2208-2217.</p>
<p>[Long et al., 2015b] Long, M., Wang, J., Sun, J., and Philip, S. Y. (2015b). Domain invari­ant transfer kernel learning. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 27(6):1519-1532.</p>
<p>[Luo et al., 2017] Luo, Z., Zou, Y., Hoffman, J., and Fei-Fei, L. F. (2017). Label efficient learning of transferable representations acrosss domains and tasks. In <em>Advances in Neural Information Processing Systems</em>, pages 164-176.</p>
<p>[Mihalkova et al., 2007] Mihalkova, L., Huynh, T., and Mooney, R. J. (2007). Mapping and revising markov logic networks for transfer learning. In <em>AAAI</em>, volume 7, pages 608-614.</p>
<p>[Mihalkova and Mooney, 2008] Mihalkova, L. and Mooney, R. J. (2008). Transfer learning by mapping with minimal target data. In <em>Proceedings of the AAAI-08 workshop on transfer learning for complex tasks</em>.</p>
<p>[Nater et al., 2011] Nater, F., Tommasi, T., Grabner, H., Van Gool, L., and Caputo, B. (2011). Transferring activities: Updating human behavior analysis. In <em>Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on</em>, pages 1737­1744, Barcelona, Spain. IEEE.</p>
<p>[Pan et al., 2008a] Pan, S. J., Kwok, J. T., and Yang, Q. (2008a). Transfer learning via dimensionality reduction. In <em>Proceedings of the 23rd AAAI conference on Artificial in­telligence</em>, volume 8, pages 677-682.</p>
<p>[Pan et al., 2008b] Pan, S. J., Shen, D., Yang, Q., and Kwok, J. T. (2008b). Transferring localization models across space. In <em>Proceedings of the 23rd AAAI Conference on Artificial Intelligence</em>, pages 1383-1388.</p>
<p>[Pan et al., 2011] Pan, S. J., Tsang, I. W., Kwok, J. T., and Yang, Q. (2011). Domain adaptation via transfer component analysis. <em>IEEE TNN</em>, 22(2):199-210.</p>
<p>[PanandYang, 2010] Pan,S.J.andYang,Q.(2010). A survey on transfer learning. IEEE TKDE*, 22(10):1345-1359.</p>
<p>[Patil and Phursule, 2013] Patil, D. M. and Phursule, R. (2013). Knowledge transfer using cost sensitive online learning classification. <em>International Journal of Science and Research</em>, pages 527-529.</p>
<p>[Razavian et al., 2014] Razavian, A. S., Azizpour, H., Sullivan, J., and Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In <em>Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference on</em>, pages 512­519. IEEE.</p>
<p>[Saito et al., 2017] Saito, K., Ushiku, Y., and Harada, T. (2017). Asymmetric tri-training for unsupervised domain adaptation. In <em>International Conference on Machine Learning</em>.</p>
<p>[Sener et al., 2016] Sener, O., Song, H. O., Saxena, A., and Savarese, S. (2016). Learning transferrable representations for unsupervised domain adaptation. In <em>Advances in Neural Information Processing Systems</em>, pages 2110-2118.</p>
<p>[Shen et al., 2018] Shen, J., Qu, Y., Zhang, W., and Yu, Y. (2018). Wasserstein distance guided representation learning for domain adaptation. In <em>AAAI</em>.</p>
<p>[Si et al., 2010] Si, S., Tao, D., and Geng, B. (2010). Bregman divergence-based regu­larization for transfer subspace learning. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 22(7):929-942.</p>
<p>[Silver et al., 2017] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. <em>Nature</em>, 550(7676):354.</p>
<p>[Stewart and Ermon, 2017] Stewart, R. and Ermon, S. (2017). Label-free supervision of neural networks with physics and domain knowledge. In <em>AAAI</em>, pages 2576-2582.</p>
<p>[Sun et al., 2016] Sun, B., Feng, J., and Saenko, K. (2016). Return of frustratingly easy domain adaptation. In <em>AAAI</em>, volume 6, page 8.</p>
<p>[Sun and Saenko, 2015] Sun, B. and Saenko, K. (2015). Subspace distribution alignment for unsupervised domain adaptation. In <em>BMVC</em>, pages 24-1.</p>
<p>[Sun and Saenko, 2016] Sun, B. and Saenko, K. (2016). Deep coral: Correlation alignment for deep domain adaptation. In <em>European Conference on Computer Vision</em>, pages 443-450. Springer.</p>
<p>[Tahmoresnezhad and Hashemi, 2016] Tahmoresnezhad, J. and Hashemi, S. (2016). Visual domain adaptation via transfer feature learning. <em>Knowledge and Information Systems</em>, pages 1-21.</p>
<p>[Tan et al., 2015] Tan, B., Song, Y., Zhong, E., and Yang, Q. (2015). Transitive trans­fer learning. In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, pages 1155-1164. ACM.</p>
<p>[Tan et al., 2017] Tan, B., Zhang, Y., Pan, S. J., and Yang, Q. (2017). Distant domain transfer learning. In <em>Thirty-First AAAI Conference on Artificial Intelligence</em>.</p>
<p>[Taylor and Stone, 2009] Taylor, M. E. and Stone, P. (2009). Transfer learning for reinforce­ment learning domains: A survey. <em>Journal of Machine Learning Research</em>, 10(Jul):1633- 1685.</p>
<p>[Tzeng et al., 2015] Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. (2015). Simulta­neous deep transfer across domains and tasks. In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, pages 4068-4076, Santiago, Chile. IEEE.</p>
<p>[Tzeng et al., 2017] Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017). Adversarial discriminative domain adaptation. In <em>CVPR</em>, pages 2962-2971.</p>
<p>[Tzeng et al., 2014] Tzeng, E., Hoffman, J., Zhang, N., et al. (2014). Deep domain confu­sion: Maximizing for domain invariance. <em>arXiv preprint arXiv:1412.3474</em>.</p>
<p>[Wang et al., 2017] Wang, J., Chen, Y., Hao, S., et al. (2017). Balanced distribution adap­tation for transfer learning. In <em>ICDM</em>, pages 1129-1134.</p>
<p>[Wang et al., 2018] Wang, J., Chen, Y., Hu, L., Peng, X., and Yu, P. S. (2018). Strati­fied transfer learning for cross-domain activity recognition. In <em>2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)</em>.</p>
<p>[Wang et al., 2014] Wang, J., Zhao, P., Hoi, S. C., and Jin, R. (2014). Online feature selection and its applications. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 26(3):698-710.</p>
<p>[Wei et al., 2016a] Wei, P., Ke, Y., and Goh, C. K. (2016a). Deep nonlinearfeature coding for unsupervised domain adaptation. In <em>IJCAI</em>, pages 2189-2195.</p>
<p>[Wei et al., 2017] Wei, Y., Zhang, Y., and Yang, Q. (2017). Learning totransfer. <em>arXiv</em> preprint arXiv:1708.05629*.</p>
<p>[Wei et al., 2016b] Wei, Y., Zhu, Y., Leung, C. W.-k., Song, Y., and Yang, Q. (2016b). Instilling social to physical: Co-regularized heterogeneous transfer learning. In <em>Thirtieth</em> AAAI Conference on Artificial Intelligence*.</p>
<p>[Weiss et al., 2016] Weiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer learning. <em>Journal of Big Data</em>, 3(1):1-40.</p>
<p>[Wu et al., 2017] Wu, Q., Zhou, X., Yan, Y., Wu, H., and Min, H. (2017). Online transfer learning by leveraging multiple source domains. <em>Knowledge and Information Systems</em>, 52(3):687-707.</p>
<p>[xinhua, 2016] xinhua (2016). <a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MjM5ODYzNzAyMQ==&amp;">http://mp.weixin.qq.com/s?__biz=MjM5ODYzNzAyMQ==&amp;</a> mid=2651933920&amp;idx=1\&amp;sn=ae2866bd12000f1644eae1094497837e.</p>
<p>[Yan et al., 2017] Yan, Y., Wu, Q., Tan, M., Ng, M. K., Min, H., and Tsang, I. W. (2017). Online heterogeneous transfer by hedge ensemble of offline and online decisions. <em>IEEE transactions on neural networks and learning systems</em>.</p>
<p>[Yosinski et al., 2014] Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. (2014). How transferable are features in deep neural networks? In <em>Advances in neural information processing systems</em>, pages 3320-3328.</p>
<p>[Zadrozny, 2004] Zadrozny, B. (2004). Learning and evaluating classifiers under sample selection bias. In <em>Proceedings of the twenty-first international conference on Machine learning</em>, page 114, Alberta, Canada. ACM.</p>
<p>[Zellinger et al., 2017] Zellinger, W., Grubinger, T., Lughofer, E., Natschlager, T., and Saminger-Platz, S. (2017). Central moment discrepancy (cmd) for domain-invariant rep­resentation learning. <em>arXiv preprint arXiv:1702.08811</em>.</p>
<p>[Zhang et al., 2017a] Zhang, J., Li, W., and Ogunbona, P. (2017a). Joint geometrical and statistical alignment for visual domain adaptation. In <em>CVPR</em>.</p>
<p>[Zhang et al., 2017b] Zhang, X., Zhuang, Y., Wang, W., and Pedrycz, W. (2017b). On­line feature transformation learning for cross-domain object category recognition. <em>IEEE transactions on neural networks and learning systems</em>.</p>
<p>[Zhao and Hoi, 2010] Zhao, P. and Hoi, S. C. (2010). Otl: A framework of online transfer learning. In <em>Proceedings of the 27th international conference on machine learning (ICML- 10)</em>, pages 1231-1238.</p>
<p>[Zhao et al., 2010] Zhao, Z., Chen, Y., Liu, J., and Liu, M. (2010). Cross-mobile elm based activity recognition. <em>International Journal of Engineering and Industries</em>, 1(1):30-38.</p>
<p>[Zhao et al., 2011] Zhao, Z., Chen, Y., Liu, J., Shen, Z., and Liu, M. (2011). Cross-people mobile-phone based activity recognition. In <em>Proceedings of the Twenty-Second interna­tional joint conference on Artificial Intelligence (IJCAI)</em>, volume 11, pages 2545-2550. Citeseer.</p>
<p>[Zheng et al., 2009] Zheng, V. W., Hu, D. H., and Yang, Q. (2009). Cross-domain activity recognition. In <em>Proceedings of the 11th international conference on Ubiquitous computing</em>, pages 61-70. ACM.</p>
<p>[Zheng et al., 2008] Zheng, V. W., Pan, S. J., Yang, Q., and Pan, J. J. (2008). Transferring multi-device localization models using latent multi-task learning. In <em>AAAI</em>, volume 8, pages 1427-1432, Chicago, Illinois, USA. AAAI.</p>
<p>[Zhuang et al., 2015] Zhuang, F., Cheng, X., Luo, P., Pan, S. J., and He, Q. (2015). Su­pervised representation learning: Transfer learning with deep autoencoders. In <em>IJCAI</em>,pages 4119-4125.</p>
<p>[Zhuo et al., 2017] Zhuo, J., Wang, S., Zhang, W., and Huang, Q. (2017). Deep unsuper­vised convolutional domain adaptation. In <em>Proceedings of the 2017 ACM on Multimedia Conference</em>, pages 261-269. ACM.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://yuanweize.github.io">GreenSeaa</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://yuanweize.github.io/2022/08/09/DeepLearning-500-questions/ch11_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0_%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">https://yuanweize.github.io/2022/08/09/DeepLearning-500-questions/ch11_迁移学习/第十一章_迁移学习/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://yuanweize.github.io" target="_blank">HExLL-迷雾日志</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning</a><a class="post-meta__tags" href="/tags/ch11-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">ch11_迁移学习</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/09/DeepLearning-500-questions/ch09_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/%E7%AC%AC%E4%B9%9D%E7%AB%A0_%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">第九章_图像分割</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/09/DeepLearning-500-questions/ch12_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0_%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA%E5%8F%8A%E8%AE%AD%E7%BB%83/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">第十二章_网络搭建及训练</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/08/09/DeepLearning-500-questions/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%B8%89%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="第三章_深度学习基础"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 09-08-2022</div><div class="title">第三章_深度学习基础</div></div></a></div><div><a href="/2022/08/09/DeepLearning-500-questions/ch04_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/%E7%AC%AC%E5%9B%9B%E7%AB%A0_%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/" title="第四章_经典网络"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 09-08-2022</div><div class="title">第四章_经典网络</div></div></a></div><div><a href="/2022/08/09/DeepLearning-500-questions/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%BA%8C%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="第二章_机器学习基础"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 09-08-2022</div><div class="title">第二章_机器学习基础</div></div></a></div><div><a href="/2022/08/09/DeepLearning-500-questions/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%B8%80%E7%AB%A0_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/" title="第一章_数学基础"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 09-08-2022</div><div class="title">第一章_数学基础</div></div></a></div><div><a href="/2022/08/09/DeepLearning-500-questions/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/%E7%AC%AC%E4%BA%94%E7%AB%A0_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/" title="第五章_卷积神经网络(CNN)"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 09-08-2022</div><div class="title">第五章_卷积神经网络(CNN)</div></div></a></div><div><a href="/2022/08/09/DeepLearning-500-questions/ch06_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(RNN)/%E7%AC%AC%E5%85%AD%E7%AB%A0_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(RNN)/" title="第六章_循环神经网络(RNN)"><img class="cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 09-08-2022</div><div class="title">第六章_循环神经网络(RNN)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s.gravatar.com/avatar/50de7ee8a1fc96ada7495a641400642d?s=512" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">GreenSeaa</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">803</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">75</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">72</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yuanweize"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/yuanweize" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:info@eurun.eu.org" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://t.me/greenseaa" target="_blank" title=""><i class="fab fa-telegram"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Here is HExLL</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">第十一章 迁移学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#11-1-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">1.1.</span> <span class="toc-text">11.1 迁移学习基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-number">1.1.1.</span> <span class="toc-text">11.1.1 什么是迁移学习？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-number">1.1.2.</span> <span class="toc-text">11.1.2 为什么需要迁移学习？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-3-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">1.1.3.</span> <span class="toc-text">11.1.3 迁移学习的基本问题有哪些？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-4-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E6%9C%89%E5%93%AA%E4%BA%9B%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5%EF%BC%9F"><span class="toc-number">1.1.4.</span> <span class="toc-text">11.1.4 迁移学习有哪些常用概念？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-5-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">1.1.5.</span> <span class="toc-text">11.1.5 迁移学习与传统机器学习有什么区别？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-6-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A0%B8%E5%BF%83%E5%8F%8A%E5%BA%A6%E9%87%8F%E5%87%86%E5%88%99%EF%BC%9F"><span class="toc-number">1.1.6.</span> <span class="toc-text">11.1.6 迁移学习的核心及度量准则？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-7-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%85%B6%E4%BB%96%E6%A6%82%E5%BF%B5%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">1.1.7.</span> <span class="toc-text">11.1.7 迁移学习与其他概念的区别？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-8-%E4%BB%80%E4%B9%88%E6%98%AF%E8%B4%9F%E8%BF%81%E7%A7%BB%EF%BC%9F%E4%BA%A7%E7%94%9F%E8%B4%9F%E8%BF%81%E7%A7%BB%E7%9A%84%E5%8E%9F%E5%9B%A0%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">1.1.8.</span> <span class="toc-text">11.1.8 什么是负迁移？产生负迁移的原因有哪些？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-9-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF%EF%BC%9F"><span class="toc-number">1.1.9.</span> <span class="toc-text">11.1.9 迁移学习的基本思路？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-2-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E8%B7%AF%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">11.2 迁移学习的基本思路有哪些？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-1-%E5%9F%BA%E4%BA%8E%E6%A0%B7%E6%9C%AC%E8%BF%81%E7%A7%BB"><span class="toc-number">1.2.1.</span> <span class="toc-text">11.2.1 基于样本迁移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-2-%E5%9F%BA%E4%BA%8E%E7%89%B9%E5%BE%81%E8%BF%81%E7%A7%BB"><span class="toc-number">1.2.2.</span> <span class="toc-text">11.2.2 基于特征迁移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-3-%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E8%BF%81%E7%A7%BB"><span class="toc-number">1.2.3.</span> <span class="toc-text">11.2.3 基于模型迁移</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-4-%E5%9F%BA%E4%BA%8E%E5%85%B3%E7%B3%BB%E8%BF%81%E7%A7%BB"><span class="toc-number">1.2.4.</span> <span class="toc-text">11.2.4 基于关系迁移</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-3-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">11.3 迁移学习的常用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-1-%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E8%87%AA%E9%80%82%E5%BA%94"><span class="toc-number">1.3.1.</span> <span class="toc-text">11.3.1 数据分布自适应</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-2-%E8%BE%B9%E7%BC%98%E5%88%86%E5%B8%83%E8%87%AA%E9%80%82%E5%BA%94"><span class="toc-number">1.3.2.</span> <span class="toc-text">11.3.2 边缘分布自适应</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-3-%E6%9D%A1%E4%BB%B6%E5%88%86%E5%B8%83%E8%87%AA%E9%80%82%E5%BA%94"><span class="toc-number">1.3.3.</span> <span class="toc-text">11.3.3 条件分布自适应</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-4-%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E8%87%AA%E9%80%82%E5%BA%94%E6%96%B9%E6%B3%95%E4%BC%98%E5%8A%A3%E6%80%A7%E6%AF%94%E8%BE%83"><span class="toc-number">1.3.4.</span> <span class="toc-text">11.3.4 概率分布自适应方法优劣性比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-5-%E7%BB%9F%E8%AE%A1%E7%89%B9%E5%BE%81%E5%AF%B9%E9%BD%90%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.5.</span> <span class="toc-text">11.3.5 统计特征对齐方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-6-%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.6.</span> <span class="toc-text">11.3.6 流形学习方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-7-%E4%BB%80%E4%B9%88%E6%98%AFfinetune%EF%BC%9F"><span class="toc-number">1.3.7.</span> <span class="toc-text">11.3.7 什么是finetune？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-8-finetune%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%95%88%EF%BC%9F"><span class="toc-number">1.3.8.</span> <span class="toc-text">11.3.8 finetune为什么有效？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-9-%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E8%87%AA%E9%80%82%E5%BA%94%EF%BC%9F"><span class="toc-number">1.3.9.</span> <span class="toc-text">11.3.9 什么是深度网络自适应？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-10-GAN%E5%9C%A8%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">1.3.10.</span> <span class="toc-text">11.3.10 GAN在迁移学习中的应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-9"><span class="toc-number">1.4.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/10/31/6-%E7%94%A8%20Vue3%20%E5%92%8C%20Django%20%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E9%A1%B9%E7%9B%AE/" title="6-用 Vue3 和 Django 快速搭建前后端分离项目">6-用 Vue3 和 Django 快速搭建前后端分离项目</a><time datetime="2022-10-31T00:59:54.000Z" title="发表于 31-10-2022 01:59:54">31-10-2022</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/10/30/3-%E6%94%BE%E5%BC%83%E7%BA%A0%E7%BB%93%E3%80%81%E6%8B%A5%E6%8A%B1%E5%A6%A5%E5%8D%8F%EF%BC%8C%E9%87%8D%E5%99%A8%E8%BD%BB%E7%94%A8%E5%B0%B1%E6%98%AF%E9%AB%98%E6%95%88@annote/" title="3-放弃纠结、拥抱妥协，重器轻用就是高效@annote">3-放弃纠结、拥抱妥协，重器轻用就是高效@annote</a><time datetime="2022-10-30T04:30:45.000Z" title="发表于 30-10-2022 05:30:45">30-10-2022</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/08/15/C%E8%AF%AD%E8%A8%80%E4%B8%8EC-%E5%AD%A6%E4%B9%A0/" title="C语言与C++学习">C语言与C++学习</a><time datetime="2022-08-14T22:39:16.000Z" title="发表于 15-08-2022 00:39:16">15-08-2022</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/08/14/Linux-command/type/" title="type">type</a><time datetime="2022-08-14T20:29:33.000Z" title="发表于 14-08-2022 22:29:33">14-08-2022</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/08/14/Linux-command/ulimit/" title="ulimit">ulimit</a><time datetime="2022-08-14T20:29:33.000Z" title="发表于 14-08-2022 22:29:33">14-08-2022</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By GreenSeaa</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>